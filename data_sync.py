#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
=== æ•°æ®åŒæ­¥æ¨¡å— ===
å®ç°Webhook + è½®è¯¢çš„æ··åˆæ•°æ®åŒæ­¥ç­–ç•¥

æ¶æ„ï¼š
- ä¸»è¦æœºåˆ¶ï¼šWebhookæ¨é€ï¼ˆå®æ—¶ï¼Œç”±SpiderFlowå‘èµ·ï¼‰
- å¤‡ç”¨æœºåˆ¶ï¼šå®šæ—¶è½®è¯¢ï¼ˆæ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡SpiderFlowçš„æœ€æ–°èŠ‚ç‚¹ï¼‰
- ç­–ç•¥ï¼šWebhookä¼˜å…ˆ + è½®è¯¢å…œåº•
- æµé‡ä¼°ç®—ï¼š~30MB/æœˆï¼ˆå¯æ¥å—èŒƒå›´ï¼‰
"""

import asyncio
import aiohttp
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import os
from pathlib import Path
import hashlib
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
import socket

logger = logging.getLogger(__name__)

# ==================== é…ç½® ====================

# SpiderFlowåç«¯URL
SPIDERFLOW_API_URL = os.environ.get("SPIDERFLOW_API_URL", "http://localhost:8001")

# è½®è¯¢é—´éš”ï¼ˆç§’ï¼‰
POLL_INTERVAL = int(os.environ.get("POLL_INTERVAL", "300"))  # é»˜è®¤5åˆ†é’Ÿ

# æœ¬åœ°èŠ‚ç‚¹æ•°æ®åº“è·¯å¾„
NODES_DB_FILE = "verified_nodes.json"
SYNC_STATE_FILE = "sync_state.json"

# å¹¶å‘é…ç½®
MAX_CONCURRENT_SYNCS = 3
REQUEST_TIMEOUT = 10  # ç§’

# ==================== åŒæ­¥çŠ¶æ€è·Ÿè¸ª ====================

class SyncState:
    """è·Ÿè¸ªåŒæ­¥çŠ¶æ€å’Œä¸Šæ¬¡æ›´æ–°çš„ä¿¡æ¯"""
    
    def __init__(self):
        self.state_file = SYNC_STATE_FILE
        self.load()
    
    def load(self):
        """ä»æ–‡ä»¶åŠ è½½åŒæ­¥çŠ¶æ€"""
        if os.path.exists(self.state_file):
            try:
                with open(self.state_file, 'r') as f:
                    state = json.load(f)
                    self.last_webhook_time = state.get("last_webhook_time")
                    self.last_poll_time = state.get("last_poll_time")
                    self.last_sync_hash = state.get("last_sync_hash")
                    self.webhook_received_count = state.get("webhook_received_count", 0)
                    self.poll_received_count = state.get("poll_received_count", 0)
                    logger.info(f"ğŸ“‹ å·²åŠ è½½åŒæ­¥çŠ¶æ€ | Webhook: {self.webhook_received_count} | è½®è¯¢: {self.poll_received_count}")
            except Exception as e:
                logger.error(f"åŠ è½½åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")
                self._init_defaults()
        else:
            self._init_defaults()
    
    def _init_defaults(self):
        """åˆå§‹åŒ–é»˜è®¤å€¼"""
        self.last_webhook_time = None
        self.last_poll_time = None
        self.last_sync_hash = None
        self.webhook_received_count = 0
        self.poll_received_count = 0
    
    def save(self):
        """ä¿å­˜åŒæ­¥çŠ¶æ€åˆ°æ–‡ä»¶"""
        try:
            with open(self.state_file, 'w') as f:
                json.dump({
                    "last_webhook_time": self.last_webhook_time,
                    "last_poll_time": self.last_poll_time,
                    "last_sync_hash": self.last_sync_hash,
                    "webhook_received_count": self.webhook_received_count,
                    "poll_received_count": self.poll_received_count,
                    "saved_at": datetime.now().isoformat()
                }, f, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜åŒæ­¥çŠ¶æ€å¤±è´¥: {e}")
    
    def record_webhook(self, data_hash: str):
        """è®°å½•Webhookæ¥æ”¶"""
        self.last_webhook_time = datetime.now().isoformat()
        self.last_sync_hash = data_hash
        self.webhook_received_count += 1
        self.save()
    
    def record_poll(self, data_hash: str):
        """è®°å½•è½®è¯¢æ¥æ”¶"""
        self.last_poll_time = datetime.now().isoformat()
        self.last_sync_hash = data_hash
        self.poll_received_count += 1
        self.save()

# ==================== å“ˆå¸Œå’Œå˜æ›´æ£€æµ‹ ====================

def calculate_nodes_hash(nodes: List[Dict]) -> str:
    """
    è®¡ç®—èŠ‚ç‚¹æ•°æ®çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹å˜æ›´
    
    å“ˆå¸ŒåŸºäºï¼š
    - èŠ‚ç‚¹URLåˆ—è¡¨
    - æ¯ä¸ªèŠ‚ç‚¹çš„å…³é”®ä¿¡æ¯ï¼ˆå›½å®¶ã€å»¶è¿Ÿã€é€Ÿåº¦ç­‰ï¼‰
    """
    if not nodes:
        return hashlib.sha256(b"").hexdigest()
    
    # æŒ‰URLæ’åºï¼Œç¡®ä¿ä¸€è‡´æ€§
    sorted_nodes = sorted(nodes, key=lambda x: x.get("url", ""))
    
    # æå–å…³é”®å­—æ®µè¿›è¡Œå“ˆå¸Œ
    key_data = []
    for node in sorted_nodes:
        key_str = f"{node.get('url')}|{node.get('country')}|{node.get('latency')}|{node.get('speed')}"
        key_data.append(key_str)
    
    combined = "\n".join(key_data)
    return hashlib.sha256(combined.encode()).hexdigest()

# ==================== èŠ‚ç‚¹å»é‡ä¸åˆå¹¶ ====================

def get_node_unique_key(node: Dict) -> str:
    """
    è·å–èŠ‚ç‚¹çš„å”¯ä¸€æ ‡è¯†key
    è§„åˆ™ï¼šprotocol + host + portï¼ˆæœ€ç²¾ç¡®çš„ç»„åˆï¼‰
    """
    protocol = node.get("protocol", "unknown").lower()
    host = node.get("host", "").lower()
    port = node.get("port", 0)
    return f"{protocol}://{host}:{port}"

def deduplicate_nodes(nodes: List[Dict]) -> List[Dict]:
    """
    å¯¹èŠ‚ç‚¹åˆ—è¡¨è¿›è¡Œå»é‡ï¼Œä¿ç•™æœ€æ–°æ•°æ®
    
    è§„åˆ™ï¼š
    1. æŒ‰ protocol+host+port è¯†åˆ«å”¯ä¸€èŠ‚ç‚¹
    2. é‡å¤èŠ‚ç‚¹æ—¶ï¼Œæ–°æ•°æ®è¦†ç›–æ—§æ•°æ®
    3. ä¿ç•™ first_seen_atï¼ˆé¦–æ¬¡å‘ç°æ—¶é—´ï¼‰
    4. æ›´æ–° last_updated_atï¼ˆæœ€åæ›´æ–°æ—¶é—´ï¼‰
    
    è¿”å›ï¼šå»é‡åçš„èŠ‚ç‚¹åˆ—è¡¨
    """
    if not nodes:
        return []
    
    deduped = {}
    now = datetime.now().isoformat()
    
    for node in nodes:
        key = get_node_unique_key(node)
        
        if key not in deduped:
            # æ–°èŠ‚ç‚¹ï¼šæ·»åŠ æ—¶é—´æˆ³å­—æ®µ
            node["first_seen_at"] = node.get("first_seen_at", now)
            node["last_updated_at"] = now
            deduped[key] = node
            logger.debug(f"âœ¨ æ–°èŠ‚ç‚¹: {key}")
        else:
            # é‡å¤èŠ‚ç‚¹ï¼šä¿ç•™first_seen_atï¼Œæ›´æ–°å…¶ä»–å­—æ®µ
            old_node = deduped[key]
            first_seen = old_node.get("first_seen_at", now)
            
            # åˆå¹¶æ–°æ—§æ•°æ®ï¼ˆæ–°æ•°æ®ä¼˜å…ˆï¼‰
            merged = {**old_node, **node}
            merged["first_seen_at"] = first_seen  # ä¿ç•™é¦–æ¬¡å‘ç°æ—¶é—´
            merged["last_updated_at"] = now
            
            deduped[key] = merged
            logger.debug(f"ğŸ”„ æ›´æ–°èŠ‚ç‚¹: {key}")
    
    result = list(deduped.values())
    logger.info(f"ğŸ“Š å»é‡ç»“æœ: {len(nodes)} â†’ {len(result)} ä¸ªèŠ‚ç‚¹")
    return result

def merge_with_local_nodes(remote_nodes: List[Dict]) -> List[Dict]:
    """
    å°†è¿œç¨‹èŠ‚ç‚¹ä¸æœ¬åœ°èŠ‚ç‚¹è¿›è¡Œåˆå¹¶
    
    è§„åˆ™ï¼š
    1. è¿œç¨‹èŠ‚ç‚¹ä¼˜å…ˆï¼ˆè¦†ç›–æœ¬åœ°æ—§æ•°æ®ï¼‰
    2. ä¿ç•™æœ¬åœ°èŠ‚ç‚¹ä¸­ä¸åœ¨è¿œç¨‹çš„èŠ‚ç‚¹ï¼ˆæ ‡è®°ä¸ºstaleï¼‰
    3. ä¿ç•™åŸå§‹çš„ first_seen_at
    """
    local_data = load_local_nodes()
    local_nodes = local_data.get("nodes", [])
    
    # å»ºç«‹æœ¬åœ°èŠ‚ç‚¹çš„keyæ˜ å°„
    local_map = {get_node_unique_key(n): n for n in local_nodes}
    
    # å»ºç«‹è¿œç¨‹èŠ‚ç‚¹çš„keyæ˜ å°„ï¼ˆå¸¦æ–°æ•°æ®ï¼‰
    remote_map = {}
    now = datetime.now().isoformat()
    
    for node in remote_nodes:
        key = get_node_unique_key(node)
        
        # ä¿ç•™æœ¬åœ°çš„first_seen_at
        if key in local_map:
            node["first_seen_at"] = local_map[key].get("first_seen_at", now)
        else:
            node["first_seen_at"] = now
        
        node["last_updated_at"] = now
        remote_map[key] = node
    
    # åˆå¹¶ï¼šè¿œç¨‹èŠ‚ç‚¹ + æœ¬åœ°ä½†å·²è¿‡æœŸçš„èŠ‚ç‚¹ï¼ˆæ ‡è®°ä¸ºstaleï¼‰
    merged_map = {**local_map, **remote_map}
    
    # æ ‡è®°åªåœ¨æœ¬åœ°å­˜åœ¨çš„èŠ‚ç‚¹ä¸º stale
    for key, node in merged_map.items():
        if key not in remote_map:
            node["is_stale"] = True
            logger.debug(f"âš ï¸ æ ‡è®°è¿‡æœŸèŠ‚ç‚¹: {key}")
    
    result = list(merged_map.values())
    logger.info(f"ğŸ”€ åˆå¹¶ç»“æœ: æœ¬åœ°{len(local_nodes)} + è¿œç¨‹{len(remote_nodes)} = {len(result)}ä¸ªèŠ‚ç‚¹")
    return result


# ==================== TTL å’Œç”Ÿå‘½å‘¨æœŸç®¡ç† ====================

def calculate_node_age(node: Dict) -> int:
    """
    è®¡ç®—èŠ‚ç‚¹å¹´é¾„ï¼ˆå¤©æ•°ï¼‰
    
    åŸºäºfirst_seen_atæ—¶é—´æˆ³
    è¿”å›å€¼ï¼š
    - 0: ä»Šå¤©æ–°å¢
    - 1: 1å¤©å‰
    - 3: 3å¤©å‰ï¼ˆéœ€è¦éªŒè¯ï¼‰
    """
    try:
        first_seen = node.get("first_seen_at")
        if not first_seen:
            return 0
        
        # è§£æISOæ ¼å¼æ—¶é—´æˆ³
        created_time = datetime.fromisoformat(first_seen.replace('Z', '+00:00'))
        now = datetime.now(created_time.tzinfo) if created_time.tzinfo else datetime.now()
        
        age = (now - created_time).days
        return max(0, age)
    except Exception as e:
        logger.debug(f"âš ï¸  è®¡ç®—èŠ‚ç‚¹å¹´é¾„å¤±è´¥: {e}")
        return 0


def mark_nodes_for_verification(nodes: List[Dict], ttl_days: int = 3) -> List[Dict]:
    """
    æ ‡è®°éœ€è¦éªŒè¯çš„èŠ‚ç‚¹
    
    è§„åˆ™ï¼š
    - age_days >= ttl_days çš„èŠ‚ç‚¹æ ‡è®°ä¸º needs_verification=True
    - ä»æœªéªŒè¯è¿‡æˆ–éªŒè¯å¤±è´¥çš„èŠ‚ç‚¹ä¹Ÿæ ‡è®°ä¸ºéœ€è¦éªŒè¯
    - è¿”å›ä¿®æ”¹åçš„èŠ‚ç‚¹åˆ—è¡¨
    """
    for node in nodes:
        # è®¡ç®—èŠ‚ç‚¹å¹´é¾„
        age_days = calculate_node_age(node)
        node["age_days"] = age_days
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦éªŒè¯
        needs_verification = (
            age_days >= ttl_days or  # è¶…è¿‡TTL
            not node.get("last_verified_at")  # ä»æœªéªŒè¯
        )
        
        node["needs_verification"] = needs_verification
        
        if needs_verification:
            logger.debug(f"ğŸ” æ ‡è®°å¾…éªŒè¯èŠ‚ç‚¹ {get_node_unique_key(node)} | å¹´é¾„{age_days}å¤©")
    
    return nodes


def apply_node_lifecycle(nodes: List[Dict], ttl_days: int = 3, max_offline_days: int = 7) -> List[Dict]:
    """
    åº”ç”¨å®Œæ•´èŠ‚ç‚¹ç”Ÿå‘½å‘¨æœŸç®¡ç†
    
    æµç¨‹ï¼š
    1. è®¡ç®—èŠ‚ç‚¹å¹´é¾„
    2. æ ‡è®°éœ€è¦éªŒè¯çš„èŠ‚ç‚¹
    3. æ ‡è®°é•¿æœŸç¦»çº¿çš„èŠ‚ç‚¹ä¸ºåˆ é™¤å€™é€‰
    
    å‚æ•°ï¼š
    - ttl_days: èŠ‚ç‚¹TTLï¼ˆå¤©ï¼‰ï¼Œè¶…è¿‡åˆ™éœ€è¦éªŒè¯
    - max_offline_days: æœ€å¤§ç¦»çº¿å¤©æ•°ï¼Œè¶…è¿‡åˆ™åˆ é™¤
    
    è¿”å›ï¼š
    - å¤„ç†åçš„èŠ‚ç‚¹åˆ—è¡¨
    """
    processed_nodes = []
    
    for node in nodes:
        # 1. è®¡ç®—å¹´é¾„
        age_days = calculate_node_age(node)
        node["age_days"] = age_days
        
        # 2. æ£€æŸ¥ç¦»çº¿çŠ¶æ€å’Œç¦»çº¿æ—¶é•¿
        offline_status = node.get("offline_status", False)
        verification_failed_at = node.get("verification_failed_at")
        
        if offline_status and verification_failed_at:
            try:
                failed_time = datetime.fromisoformat(verification_failed_at.replace('Z', '+00:00'))
                now = datetime.now(failed_time.tzinfo) if failed_time.tzinfo else datetime.now()
                offline_days = (now - failed_time).days
                
                # ç¦»çº¿è¶…è¿‡max_offline_daysåˆ™æ ‡è®°åˆ é™¤
                if offline_days > max_offline_days:
                    node["should_delete"] = True
                    logger.info(f"ğŸ—‘ï¸  æ ‡è®°åˆ é™¤é•¿æœŸç¦»çº¿èŠ‚ç‚¹ {get_node_unique_key(node)} | ç¦»çº¿{offline_days}å¤©")
                    continue  # ä¸åŠ å…¥è¿”å›åˆ—è¡¨
            except Exception as e:
                logger.debug(f"âš ï¸  æ£€æŸ¥ç¦»çº¿æ—¶é•¿å¤±è´¥: {e}")
        
        # 3. æ ‡è®°éœ€è¦éªŒè¯çš„èŠ‚ç‚¹
        needs_verification = (
            age_days >= ttl_days or
            not node.get("last_verified_at") or
            offline_status
        )
        node["needs_verification"] = needs_verification
        
        processed_nodes.append(node)
    
    return processed_nodes


# ==================== èŠ‚ç‚¹æ´»åŠ›éªŒè¯ ====================

async def verify_node_connectivity(node: Dict, timeout: int = 5) -> bool:
    """
    éªŒè¯èŠ‚ç‚¹è¿é€šæ€§
    
    å®ç°æ–¹å¼ï¼š
    - é€šè¿‡HTTP HEADè¯·æ±‚éªŒè¯ä»£ç†æ˜¯å¦å¯è¾¾
    - æµ‹è¯•åœ°å€ï¼šhttps://www.cloudflare.com/ï¼ˆè½»é‡çº§ï¼‰
    - æ”¯æŒHTTPä»£ç†åè®®
    
    å‚æ•°ï¼š
    - node: èŠ‚ç‚¹ä¿¡æ¯å­—å…¸
    - timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    
    è¿”å›ï¼š
    - True: èŠ‚ç‚¹å¯è¾¾
    - False: èŠ‚ç‚¹ä¸å¯è¾¾
    """
    try:
        proxy_url = node.get("proxy_url") or f"{node.get('protocol', 'http')}://{node.get('host')}:{node.get('port')}"
        
        # è®¾ç½®ä»£ç†åè®®
        proxy_protocol = node.get('protocol', 'http').lower()
        if proxy_protocol not in ['http', 'https', 'socks5']:
            return False
        
        # æ„å»ºä»£ç†URL (HTTPé€šç”¨æ ¼å¼)
        auth_str = ""
        if node.get('username') and node.get('password'):
            auth_str = f"{node.get('username')}:{node.get('password')}@"
        
        proxy_connect_url = f"{proxy_protocol}://{auth_str}{node.get('host')}:{node.get('port')}"
        
        # ä½¿ç”¨aiohttpéªŒè¯è¿é€šæ€§
        async with aiohttp.ClientSession() as session:
            async with session.head(
                'https://www.cloudflare.com/',
                proxy=proxy_connect_url if proxy_protocol != 'socks5' else None,
                timeout=aiohttp.ClientTimeout(total=timeout),
                ssl=False
            ) as resp:
                return resp.status < 500  # åªè¦ä¸æ˜¯5xxé”™è¯¯å°±è®¤ä¸ºå¯è¾¾
                
    except asyncio.TimeoutError:
        logger.debug(f"â±ï¸  èŠ‚ç‚¹éªŒè¯è¶…æ—¶: {get_node_unique_key(node)}")
        return False
    except Exception as e:
        logger.debug(f"âŒ èŠ‚ç‚¹éªŒè¯å¤±è´¥ {get_node_unique_key(node)}: {e}")
        return False


def mark_node_offline(node: Dict) -> Dict:
    """
    æ ‡è®°èŠ‚ç‚¹ä¸ºç¦»çº¿çŠ¶æ€
    
    æ›´æ–°å­—æ®µï¼š
    - offline_status: æ ‡è®°ä¸ºTrue
    - verification_failed_at: è®°å½•å¤±è´¥æ—¶é—´
    - last_verified_at: æ›´æ–°ä¸ºå½“å‰æ—¶é—´
    """
    node["offline_status"] = True
    node["verification_failed_at"] = datetime.now().isoformat()
    node["last_verified_at"] = datetime.now().isoformat()
    return node


async def verify_nodes_batch(nodes_to_verify: List[Dict]) -> List[Dict]:
    """
    æ‰¹é‡éªŒè¯èŠ‚ç‚¹
    
    æµç¨‹ï¼š
    1. éå†å¾…éªŒè¯èŠ‚ç‚¹åˆ—è¡¨
    2. é€ä¸ªæ‰§è¡Œè¿é€šæ€§æµ‹è¯•
    3. å¤±è´¥çš„èŠ‚ç‚¹æ ‡è®°ä¸ºç¦»çº¿
    4. æ›´æ–°last_verified_atæ—¶é—´æˆ³
    
    è¿”å›ï¼š
    - åŒ…å«éªŒè¯ç»“æœçš„èŠ‚ç‚¹åˆ—è¡¨
    """
    verified = []
    failed_count = 0
    
    for node in nodes_to_verify:
        node_key = get_node_unique_key(node)
        is_reachable = await verify_node_connectivity(node)
        
        if is_reachable:
            node["offline_status"] = False
            node["last_verified_at"] = datetime.now().isoformat()
            logger.info(f"âœ… èŠ‚ç‚¹éªŒè¯é€šè¿‡: {node_key}")
        else:
            mark_node_offline(node)
            failed_count += 1
            logger.warning(f"âŒ èŠ‚ç‚¹éªŒè¯å¤±è´¥: {node_key}")
        
        verified.append(node)
        # æ¯ä¸ªéªŒè¯é—´éš”100msï¼Œé¿å…è¿‡äºæ¿€è¿›
        await asyncio.sleep(0.1)
    
    logger.info(f"ğŸ” æ‰¹é‡éªŒè¯å®Œæˆ | æ€»è®¡{len(verified)}ä¸ª | å¤±è´¥{failed_count}ä¸ª")
    return verified


async def scheduled_node_verification():
    """
    å®šæ—¶ä»»åŠ¡ï¼šéªŒè¯3å¤©ä»¥ä¸Šçš„èŠ‚ç‚¹
    
    æ‰§è¡Œé¢‘ç‡ï¼šæ¯å¤©å‡Œæ™¨2:00
    è§„åˆ™ï¼š
    1. åŠ è½½æœ¬åœ°èŠ‚ç‚¹æ•°æ®
    2. ç­›é€‰age_days >= 3ä¸”needs_verification=Trueçš„èŠ‚ç‚¹
    3. æ‰¹é‡éªŒè¯è¿™äº›èŠ‚ç‚¹
    4. åˆ é™¤é•¿æœŸç¦»çº¿çš„èŠ‚ç‚¹
    5. ä¿å­˜æ›´æ–°ç»“æœ
    """
    logger.info("ğŸ”„ å¼€å§‹å®šæ—¶èŠ‚ç‚¹éªŒè¯ä»»åŠ¡ (æ¯å¤©2:00æ‰§è¡Œ)")
    
    try:
        data = load_local_nodes()
        nodes = data.get("nodes", [])
        
        if not nodes:
            logger.info("ğŸ“Š æ— èŠ‚ç‚¹æ•°æ®ï¼Œè·³è¿‡éªŒè¯")
            return
        
        # 1. ç­›é€‰éœ€è¦éªŒè¯çš„èŠ‚ç‚¹ (age_days >= 3)
        nodes_to_verify = [
            n for n in nodes 
            if n.get("needs_verification") and n.get("age_days", 0) >= 3
        ]
        
        if not nodes_to_verify:
            logger.info("âœ… æ— éœ€éªŒè¯çš„èŠ‚ç‚¹")
            return
        
        logger.info(f"ğŸ” å‡†å¤‡éªŒè¯{len(nodes_to_verify)}ä¸ªèŠ‚ç‚¹...")
        
        # 2. æ‰¹é‡éªŒè¯
        verified_nodes = await verify_nodes_batch(nodes_to_verify)
        
        # 3. æ›´æ–°èŠ‚ç‚¹æ˜ å°„
        verified_map = {get_node_unique_key(n): n for n in verified_nodes}
        for i, node in enumerate(nodes):
            key = get_node_unique_key(node)
            if key in verified_map:
                nodes[i] = verified_map[key]
        
        # 4. åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆåˆ é™¤é•¿æœŸç¦»çº¿çš„ï¼‰
        final_nodes = apply_node_lifecycle(nodes, ttl_days=3, max_offline_days=7)
        
        # 5. ä¿å­˜æ›´æ–°ç»“æœ
        data["nodes"] = final_nodes
        data["last_verified_at"] = datetime.now().isoformat()
        save_local_nodes(data)
        
        # 6. è®°å½•ç»Ÿè®¡
        offline_count = len([n for n in final_nodes if n.get("offline_status")])
        logger.info(f"âœ… èŠ‚ç‚¹éªŒè¯ä»»åŠ¡å®Œæˆ | æ€»è®¡{len(final_nodes)}ä¸ª | ç¦»çº¿{offline_count}ä¸ª")
        
    except Exception as e:
        logger.error(f"âŒ èŠ‚ç‚¹éªŒè¯ä»»åŠ¡å¼‚å¸¸: {e}")


# ==================== ç½‘ç»œè¯·æ±‚ ====================

async def fetch_nodes_from_spiderflow() -> Optional[Dict[str, Any]]:
    """
    ä»SpiderFlow APIè·å–æœ€æ–°çš„èŠ‚ç‚¹åˆ—è¡¨
    
    ç«¯ç‚¹ï¼šGET /nodes/export?format=json
    è¿”å›ï¼š{ "nodes": [...], "last_updated": "...", "total_count": ... }
    """
    try:
        async with aiohttp.ClientSession() as session:
            url = f"{SPIDERFLOW_API_URL}/nodes/export?format=json"
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=REQUEST_TIMEOUT)) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    logger.info(f"âœ… ä»SpiderFlowè·å–{len(data.get('nodes', []))}ä¸ªèŠ‚ç‚¹")
                    return data
                else:
                    logger.error(f"SpiderFlowè¿”å›é”™è¯¯çŠ¶æ€: {resp.status}")
                    return None
    except asyncio.TimeoutError:
        logger.error(f"âŒ è¿æ¥SpiderFlowè¶…æ—¶")
        return None
    except Exception as e:
        logger.error(f"âŒ ä»SpiderFlowè·å–æ•°æ®å¤±è´¥: {e}")
        return None

# ==================== æœ¬åœ°å­˜å‚¨æ“ä½œ ====================

def load_local_nodes() -> Dict[str, Any]:
    """ä»æœ¬åœ°åŠ è½½èŠ‚ç‚¹æ•°æ®"""
    if os.path.exists(NODES_DB_FILE):
        try:
            with open(NODES_DB_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"åŠ è½½æœ¬åœ°èŠ‚ç‚¹å¤±è´¥: {e}")
            return {"nodes": []}
    return {"nodes": []}

def save_local_nodes(data: Dict[str, Any]):
    """ä¿å­˜èŠ‚ç‚¹æ•°æ®åˆ°æœ¬åœ°"""
    try:
        with open(NODES_DB_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ å·²ä¿å­˜{len(data.get('nodes', []))}ä¸ªèŠ‚ç‚¹åˆ°æœ¬åœ°")
    except Exception as e:
        logger.error(f"ä¿å­˜æœ¬åœ°èŠ‚ç‚¹å¤±è´¥: {e}")

# ==================== æ ¸å¿ƒåŒæ­¥é€»è¾‘ ====================

async def poll_spiderflow_nodes() -> Optional[Dict[str, Any]]:
    """
    è½®è¯¢ä»SpiderFlowè·å–èŠ‚ç‚¹æ•°æ®
    
    å·¥ä½œæµç¨‹ï¼š
    1. è¿æ¥SpiderFlow API
    2. è·å–æœ€æ–°èŠ‚ç‚¹åˆ—è¡¨
    3. æ‰§è¡Œå»é‡å’Œåˆå¹¶
    4. ä¸æœ¬åœ°æ•°æ®å¯¹æ¯”ï¼ˆå“ˆå¸Œæ£€æŸ¥ï¼‰
    5. å¦‚æœæœ‰å˜æ›´ï¼Œæ›´æ–°æœ¬åœ°æ•°æ®åº“
    6. è®°å½•åŒæ­¥çŠ¶æ€
    """
    logger.info("ğŸ”„ å¼€å§‹è½®è¯¢SpiderFlowèŠ‚ç‚¹...")
    
    # è·å–è¿œç¨‹èŠ‚ç‚¹æ•°æ®
    remote_data = await fetch_nodes_from_spiderflow()
    if not remote_data:
        logger.warning("âš ï¸ è½®è¯¢å¤±è´¥ï¼Œä½¿ç”¨æœ¬åœ°æ•°æ®")
        return None
    
    remote_nodes = remote_data.get("nodes", [])
    logger.info(f"ğŸ“¥ ä»SpiderFlowè·å–{len(remote_nodes)}ä¸ªèŠ‚ç‚¹")
    
    # 1. è¿œç¨‹èŠ‚ç‚¹å»é‡
    deduplicated = deduplicate_nodes(remote_nodes)
    
    # 2. ä¸æœ¬åœ°èŠ‚ç‚¹åˆå¹¶
    merged_nodes = merge_with_local_nodes(deduplicated)
    
    # è®¡ç®—å“ˆå¸Œå€¼
    merged_hash = calculate_nodes_hash(merged_nodes)
    
    # åŠ è½½æœ¬åœ°æ•°æ®è·å–æ—§å“ˆå¸Œ
    local_data = load_local_nodes()
    local_nodes = local_data.get("nodes", [])
    local_hash = calculate_nodes_hash(local_nodes)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å˜æ›´
    if merged_hash == local_hash and local_nodes:
        logger.info("ğŸ“Š èŠ‚ç‚¹æ•°æ®æ— å˜æ›´ï¼Œè·³è¿‡æ›´æ–°")
        return None
    
    # æ›´æ–°æœ¬åœ°æ•°æ®
    updated_data = {
        "nodes": merged_nodes,
        "last_synced_from": "spiderflow_poll",
        "last_synced_at": datetime.now().isoformat(),
        "nodes_count": len([n for n in merged_nodes if not n.get("is_stale")]),
        "sync_metadata": {
            "total_count": len(merged_nodes),
            "active_count": len([n for n in merged_nodes if not n.get("is_stale")]),
            "remote_timestamp": remote_data.get("last_updated"),
            "deduplicated": len(remote_nodes) - len(deduplicated)
        }
    }
    
    # 3. åº”ç”¨TTLå’Œç”Ÿå‘½å‘¨æœŸç®¡ç†
    lifecycle_nodes = apply_node_lifecycle(updated_data["nodes"])
    needs_verification_count = len([n for n in lifecycle_nodes if n.get("needs_verification")])
    
    updated_data["nodes"] = lifecycle_nodes
    updated_data["sync_metadata"]["needs_verification"] = needs_verification_count
    
    save_local_nodes(updated_data)
    
    # è®°å½•è½®è¯¢çŠ¶æ€
    sync_state = SyncState()
    sync_state.record_poll(merged_hash)
    
    logger.info(f"âœ… è½®è¯¢å®Œæˆ | æ€»è®¡{len(lifecycle_nodes)}ä¸ªèŠ‚ç‚¹ | æ´»è·ƒ{updated_data['nodes_count']}ä¸ª | å¾…éªŒè¯{needs_verification_count}ä¸ª")
    return updated_data

async def handle_webhook_sync(webhook_payload: Dict[str, Any]):
    """
    å¤„ç†WebhookåŒæ­¥ï¼ˆå®æ—¶ï¼Œç”±SpiderFlowå‘èµ·ï¼‰
    
    å·¥ä½œæµç¨‹ï¼š
    1. æ¥æ”¶webhookæ¨é€çš„èŠ‚ç‚¹
    2. æ‰§è¡Œå»é‡å’Œåˆå¹¶
    3. æ›´æ–°æœ¬åœ°æ•°æ®åº“
    4. è®°å½•åŒæ­¥çŠ¶æ€
    
    ä¼˜åŠ¿ï¼š
    - å®æ—¶æ¨é€ï¼Œæ— å»¶è¿Ÿ
    - æœ€å°æµé‡å¼€é”€
    - å“åº”æ—¶é—´çŸ­
    """
    try:
        remote_nodes = webhook_payload.get("nodes", [])
        logger.info(f"âš¡ æ¥æ”¶Webhookæ¨é€ï¼Œ{len(remote_nodes)}ä¸ªèŠ‚ç‚¹")
        
        # 1. è¿œç¨‹èŠ‚ç‚¹å»é‡
        deduplicated = deduplicate_nodes(remote_nodes)
        
        # 2. ä¸æœ¬åœ°èŠ‚ç‚¹åˆå¹¶
        merged_nodes = merge_with_local_nodes(deduplicated)
        
        # è®¡ç®—å“ˆå¸Œ
        merged_hash = calculate_nodes_hash(merged_nodes)
        
        # æ›´æ–°æœ¬åœ°æ•°æ®
        data = {
            "nodes": merged_nodes,
            "last_synced_from": "webhook",
            "last_synced_at": datetime.now().isoformat(),
            "nodes_count": len([n for n in merged_nodes if not n.get("is_stale")]),
            "sync_metadata": {
                "total_count": len(merged_nodes),
                "active_count": len([n for n in merged_nodes if not n.get("is_stale")]),
                "deduplicated": len(remote_nodes) - len(deduplicated)
            }
        }
        save_local_nodes(data)
        
        # è®°å½•çŠ¶æ€
        sync_state = SyncState()
        sync_state.record_webhook(merged_hash)
        
        logger.info(f"âš¡ WebhookåŒæ­¥å®Œæˆ | æ€»è®¡{len(merged_nodes)}ä¸ªèŠ‚ç‚¹ | æ´»è·ƒ{data['nodes_count']}ä¸ª")
        
    except Exception as e:
        logger.error(f"WebhookåŒæ­¥å¤„ç†å¤±è´¥: {e}")

# ==================== å®šæ—¶è°ƒåº¦å™¨ ====================

class DataSyncScheduler:
    """ç®¡ç†å®šæ—¶è½®è¯¢ä»»åŠ¡å’ŒèŠ‚ç‚¹éªŒè¯ä»»åŠ¡"""
    
    def __init__(self):
        self.running = False
        self.poll_task = None
        self.scheduler = None
    
    async def start(self):
        """å¯åŠ¨å®šæ—¶è½®è¯¢"""
        self.running = True
        logger.info(f"ğŸš€ å¯åŠ¨æ•°æ®åŒæ­¥è°ƒåº¦å™¨ | è½®è¯¢é—´éš”: {POLL_INTERVAL}ç§’")
        
        # å¯åŠ¨APSchedulerè¿›è¡Œå®šæ—¶éªŒè¯
        if not self.scheduler:
            self.scheduler = BackgroundScheduler()
            # æ¯å¤©å‡Œæ™¨2:00æ‰§è¡ŒèŠ‚ç‚¹éªŒè¯
            self.scheduler.add_job(
                scheduled_node_verification,
                CronTrigger(hour=2, minute=0, timezone='Asia/Shanghai'),
                id='node_verification',
                name='Daily Node Verification',
                replace_existing=True
            )
            self.scheduler.start()
            logger.info("âœ… èŠ‚ç‚¹éªŒè¯å®šæ—¶å™¨å·²å¯åŠ¨ (æ¯å¤©2:00æ‰§è¡Œ)")
        
        while self.running:
            try:
                # æ‰§è¡Œè½®è¯¢
                await poll_spiderflow_nodes()
                
                # ç­‰å¾…ä¸‹ä¸€ä¸ªè½®è¯¢å‘¨æœŸ
                await asyncio.sleep(POLL_INTERVAL)
                
            except Exception as e:
                logger.error(f"è½®è¯¢å¾ªç¯å¼‚å¸¸: {e}")
                await asyncio.sleep(10)  # å‡ºé”™åç­‰å¾…10ç§’å†é‡è¯•
    
    async def stop(self):
        """åœæ­¢å®šæ—¶è½®è¯¢"""
        self.running = False
        if self.poll_task:
            self.poll_task.cancel()
        
        if self.scheduler and self.scheduler.running:
            self.scheduler.shutdown()
            logger.info("ğŸ›‘ èŠ‚ç‚¹éªŒè¯å®šæ—¶å™¨å·²åœæ­¢")
        
        logger.info("ğŸ›‘ å·²åœæ­¢æ•°æ®åŒæ­¥è°ƒåº¦å™¨")

# ==================== å¿«é€Ÿå¯¼å‡ºæ¥å£ ====================

def get_exported_nodes(format: str = "json") -> str:
    """
    å¯¼å‡ºæœ¬åœ°èŠ‚ç‚¹æ•°æ®
    
    æ”¯æŒçš„æ ¼å¼ï¼š
    - json: JSONæ ¼å¼
    - clash: Clashé…ç½®æ ¼å¼
    - subscription: è®¢é˜…é“¾æ¥æ ¼å¼
    """
    data = load_local_nodes()
    nodes = data.get("nodes", [])
    
    if format == "json":
        return json.dumps({
            "nodes": nodes,
            "total_count": len(nodes),
            "exported_at": datetime.now().isoformat()
        }, ensure_ascii=False, indent=2)
    
    elif format == "clash":
        # TODO: å®ç°Clashæ ¼å¼å¯¼å‡º
        return "# Clashé…ç½®ï¼ˆå¾…å®ç°ï¼‰"
    
    elif format == "subscription":
        # TODO: å®ç°è®¢é˜…æ ¼å¼å¯¼å‡º
        return "# è®¢é˜…é“¾æ¥ï¼ˆå¾…å®ç°ï¼‰"
    
    return ""

# ==================== ç»Ÿè®¡å’Œç›‘æ§ ====================

def get_sync_statistics() -> Dict[str, Any]:
    """è·å–åŒæ­¥ç»Ÿè®¡ä¿¡æ¯"""
    sync_state = SyncState()
    local_data = load_local_nodes()
    
    return {
        "total_nodes": len(local_data.get("nodes", [])),
        "last_synced_at": local_data.get("last_synced_at"),
        "sync_method": local_data.get("last_synced_from", "unknown"),
        "webhook_syncs": sync_state.webhook_received_count,
        "poll_syncs": sync_state.poll_received_count,
        "total_syncs": sync_state.webhook_received_count + sync_state.poll_received_count,
        "last_webhook_time": sync_state.last_webhook_time,
        "last_poll_time": sync_state.last_poll_time,
        "data_hash": calculate_nodes_hash(local_data.get("nodes", []))
    }
