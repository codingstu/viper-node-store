#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
=== viper-node-store FastAPIä¸»åº”ç”¨ï¼ˆSupabaseç‰ˆæœ¬ï¼‰ ===

æ ¸å¿ƒåŠŸèƒ½ï¼š
1. ä» Supabase æ•°æ®åº“è¯»å–èŠ‚ç‚¹æ•°æ®
2. æä¾›èŠ‚ç‚¹æŸ¥è¯¢å’Œè¿‡æ»¤ API
3. æä¾›åŒæ­¥ä¿¡æ¯æŸ¥è¯¢
4. æ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ç²¾ç¡®æµ‹é€Ÿ

æ•°æ®æ¥æºï¼š
- æ‰€æœ‰èŠ‚ç‚¹æ•°æ®å­˜å‚¨åœ¨ Supabase public.nodes è¡¨
- SpiderFlow è´Ÿè´£æµ‹é€Ÿï¼Œç»“æœç›´æ¥å†™å…¥ Supabase
- viper-node-store ä»…è¯»å–å’Œå±•ç¤ºæ•°æ®

é›†æˆçš„æŠ€æœ¯æ ˆï¼š
- FastAPI: Webæ¡†æ¶
- Pydantic: æ•°æ®éªŒè¯
- Supabase: æ•°æ®åº“
- aiohttp: å¼‚æ­¥HTTPè¯·æ±‚
"""

from fastapi import FastAPI, Query, HTTPException, Header
from fastapi.responses import JSONResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import logging
import os
from datetime import datetime, timedelta
from typing import Optional
import json
import aiohttp
import asyncio
from typing import List, Dict, Optional
import time
from apscheduler.schedulers.asyncio import AsyncIOScheduler
import supabase

# ==================== é…ç½® ====================

# Supabase é…ç½®
SUPABASE_URL = os.environ.get("SUPABASE_URL", "https://hnlkwtkxbqiakeyienok.supabase.co")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY", "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImhubGt3dGt4YnFpYWtleWllbm9rIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NjY5MDQwNTksImV4cCI6MjA4MjQ4MDA1OX0.Xg9vQdUfBdUW-IJaomEIRGsX6tB_k2grhrF4dm_aNME")

# SpiderFlow åç«¯ URLï¼ˆç”¨äºåŒæ­¥çŠ¶æ€æŸ¥è¯¢ï¼Œä¸ç”¨äºè·å–èŠ‚ç‚¹ï¼‰
SPIDERFLOW_API_URL = os.environ.get("SPIDERFLOW_API_URL", "http://localhost:8001")

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)

# ==================== Pydantic Models ====================

class PrecisionTestRequest(BaseModel):
    """ç²¾ç¡®æµ‹é€Ÿè¯·æ±‚æ¨¡å‹"""
    proxy_url: str
    test_file_size: int = 50

class LatencyTestRequest(BaseModel):
    """å»¶è¿Ÿæµ‹é€Ÿè¯·æ±‚æ¨¡å‹"""
    proxy_url: str

class HealthCheckRequest(BaseModel):
    """å¥åº·æ£€æµ‹è¯·æ±‚æ¨¡å‹"""
    batch_size: int = 50  # æ¯æ‰¹æ£€æµ‹èŠ‚ç‚¹æ•°é‡ï¼ŒVercel é™åˆ¶å»ºè®® 30-50

# ==================== FastAPI åº”ç”¨ ====================

app = FastAPI(
    title="viper-node-store API",
    description="èŠ‚ç‚¹æ•°æ®ç®¡ç†å’Œå±•ç¤ºå¹³å°ï¼ˆæ•°æ®æ¥æº: Supabaseï¼‰",
    version="2.0.0"
)

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==================== é™æ€æ–‡ä»¶å’Œæ ¹è·¯ç”± ====================
# æŒ‚è½½é™æ€æ–‡ä»¶ç›®å½•ï¼Œä½†åªä¸º /static è·¯ç”±æä¾›æ–‡ä»¶
import os
static_dir = os.path.join(os.path.dirname(__file__))
app.mount("/static", StaticFiles(directory=static_dir), name="static")

# ==================== Supabase è¾…åŠ©å‡½æ•° ====================

async def get_supabase_nodes(
    limit: int = 500,
    show_free: bool = True,
    show_china: bool = True
) -> List[Dict]:
    """
    ä» Supabase è·å–èŠ‚ç‚¹æ•°æ®
    
    Args:
        limit: è¿”å›çš„æœ€å¤§èŠ‚ç‚¹æ•°
        show_free: æ˜¯å¦æ˜¾ç¤ºå…è´¹èŠ‚ç‚¹
        show_china: æ˜¯å¦æ˜¾ç¤ºä¸­å›½èŠ‚ç‚¹
    
    Returns:
        èŠ‚ç‚¹åˆ—è¡¨
    """
    try:
        # æ„é€  Supabase REST API æŸ¥è¯¢ URL
        url = f"{SUPABASE_URL}/rest/v1/nodes?select=*&limit={limit}"
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if not show_free:
            url += "&is_free=eq.false"
        
        headers = {
            "apikey": SUPABASE_KEY,
            "Authorization": f"Bearer {SUPABASE_KEY}",
            "Content-Type": "application/json"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                if resp.status == 200:
                    raw_nodes = await resp.json()
                    
                    # è§£æèŠ‚ç‚¹æ•°æ®
                    nodes = []
                    for row in raw_nodes:
                        try:
                            # content å­—æ®µæ˜¯ JSONBï¼ŒåŒ…å«å®Œæ•´çš„èŠ‚ç‚¹ä¿¡æ¯
                            node_content = row.get("content", {})
                            if isinstance(node_content, str):
                                node_content = json.loads(node_content)
                            
                            # ç»„è£…èŠ‚ç‚¹å¯¹è±¡
                            node = {
                                "id": row.get("id", ""),
                                "protocol": node_content.get("protocol", ""),
                                "host": node_content.get("host", ""),
                                "port": node_content.get("port", 0),
                                "name": node_content.get("name", f"{node_content.get('host')}:{node_content.get('port')}"),
                                "country": node_content.get("country", "UNK"),
                                "link": row.get("link", "") or node_content.get("link", ""),  # ä¼˜å…ˆä»è¡¨å­—æ®µè¯»å–ï¼Œå¤‡ç”¨ä» content è¯»å–
                                "is_free": row.get("is_free", False),
                                "speed": row.get("speed", 0),
                                "latency": row.get("latency", 9999),
                                "updated_at": row.get("updated_at"),
                                "mainland_score": row.get("mainland_score", 0),
                                "mainland_latency": row.get("mainland_latency", 9999),
                                "overseas_score": row.get("overseas_score", 0),
                                "overseas_latency": row.get("overseas_latency", 9999),
                                # å¥åº·æ£€æµ‹å­—æ®µ
                                "status": row.get("status", "online"),  # èŠ‚ç‚¹çŠ¶æ€ï¼šonline/suspect/offline
                                "last_health_check": row.get("last_health_check"),
                                "health_latency": row.get("health_latency"),
                                # è®¡ç®—æ´»è·ƒçŠ¶æ€ï¼šlatency < 9999 è¡¨ç¤ºå·²æµ‹è¯•
                                "alive": row.get("latency", 9999) < 9999
                            }
                            nodes.append(node)
                        except Exception as e:
                            logger.warning(f"è§£æèŠ‚ç‚¹æ•°æ®å¤±è´¥: {e}")
                            continue
                    
                    logger.info(f"âœ… ä» Supabase è·å– {len(nodes)} ä¸ªèŠ‚ç‚¹")
                    return nodes
                else:
                    logger.error(f"âŒ Supabase è¿”å›é”™è¯¯: {resp.status}")
                    return []
    except Exception as e:
        logger.error(f"âŒ è·å– Supabase èŠ‚ç‚¹å¤±è´¥: {e}")
        return []

async def get_latest_sync_time() -> Optional[str]:
    """
    ä» Supabase è·å–æœ€åä¸€æ¬¡æ›´æ–°æ—¶é—´ï¼ˆæ‰€æœ‰èŠ‚ç‚¹ä¸­çš„æœ€æ–° updated_atï¼‰
    """
    try:
        url = f"{SUPABASE_URL}/rest/v1/nodes?select=updated_at&order=updated_at.desc&limit=1"
        
        headers = {
            "apikey": SUPABASE_KEY,
            "Authorization": f"Bearer {SUPABASE_KEY}",
            "Content-Type": "application/json"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                if resp.status == 200:
                    data = await resp.json()
                    if data and len(data) > 0:
                        return data[0].get("updated_at")
        return None
    except Exception as e:
        logger.warning(f"âš ï¸  è·å–æœ€åæ›´æ–°æ—¶é—´å¤±è´¥: {e}")
        return None

async def check_user_vip_status(user_id: Optional[str]) -> bool:
    """
    æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æ˜¯ VIP
    
    Args:
        user_id: Supabase ç”¨æˆ· ID
    
    Returns:
        True å¦‚æœæ˜¯ VIPï¼ŒFalse å¦‚æœä¸æ˜¯æˆ–ç”¨æˆ·ä¸å­˜åœ¨
    """
    if not user_id:
        return False
    
    try:
        supabase_client = supabase.create_client(SUPABASE_URL, SUPABASE_KEY)
        result = supabase_client.table("profiles").select("vip_until").eq("id", user_id).execute()
        
        if result.data and len(result.data) > 0:
            vip_until = result.data[0].get("vip_until")
            if vip_until:
                try:
                    vip_until_dt = datetime.fromisoformat(vip_until.replace("Z", "+00:00"))
                    now = datetime.now(vip_until_dt.tzinfo) if vip_until_dt.tzinfo else datetime.now()
                    return vip_until_dt > now
                except:
                    return False
        return False
    except Exception as e:
        logger.warning(f"âš ï¸  æ£€æŸ¥ VIP çŠ¶æ€å¤±è´¥: {e}")
        return False

# ==================== å¯åŠ¨å’Œå…³é—­ ====================

# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
scheduler = None

async def periodic_pull_from_supabase():
    """
    å®šæ—¶æ‹‰å–ä»»åŠ¡ï¼šæ¯12åˆ†é’Ÿä» Supabase æ‹‰å–ä¸€æ¬¡æœ€æ–°çš„èŠ‚ç‚¹æ•°æ®
    è¿™å¯ä»¥ç¡®ä¿ viper-node-store çš„å†…å­˜ç¼“å­˜ä¿æŒæœ€æ–°
    """
    try:
        logger.info("ğŸ“¥ å¼€å§‹å®šæ—¶æ‹‰å– Supabase èŠ‚ç‚¹æ•°æ®...")
        nodes = await get_supabase_nodes(limit=10000)
        logger.info(f"âœ… å®šæ—¶æ‹‰å–å®Œæˆï¼šè·å– {len(nodes)} ä¸ªèŠ‚ç‚¹")
    except Exception as e:
        logger.warning(f"âš ï¸  å®šæ—¶æ‹‰å–å¤±è´¥: {e}")

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶çš„åˆå§‹åŒ–"""
    global scheduler
    
    logger.info("=" * 60)
    logger.info("ğŸš€ viper-node-store æ­£åœ¨å¯åŠ¨...")
    logger.info("ğŸ“Š æ•°æ®æ¥æº: Supabase public.nodes è¡¨")
    logger.info("=" * 60)
    
    # éªŒè¯ Supabase è¿æ¥
    try:
        nodes = await get_supabase_nodes(limit=1)
        logger.info("âœ… Supabase è¿æ¥æˆåŠŸ")
    except Exception as e:
        logger.warning(f"âš ï¸  Supabase è¿æ¥å¤±è´¥: {e}")
    
    # å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
    try:
        scheduler = AsyncIOScheduler()
        
        # æ·»åŠ å®šæ—¶ä»»åŠ¡ï¼šæ¯12åˆ†é’Ÿæ‹‰å–ä¸€æ¬¡ Supabase æ•°æ®
        scheduler.add_job(
            periodic_pull_from_supabase,
            'interval',
            minutes=12,
            id='supabase_pull',
            name='Supabase å®šæ—¶æ‹‰å–'
        )
        
        scheduler.start()
        logger.info("âœ… å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨ï¼ˆæ¯12åˆ†é’Ÿæ‹‰å–ä¸€æ¬¡ Supabase æ•°æ®ï¼‰")
    except Exception as e:
        logger.warning(f"âš ï¸  å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å¤±è´¥: {e}")

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶çš„æ¸…ç†"""
    global scheduler
    
    logger.info("ğŸ›‘ viper-node-store æ­£åœ¨å…³é—­...")
    
    # å…³é—­è°ƒåº¦å™¨
    if scheduler and scheduler.running:
        scheduler.shutdown()
        logger.info("âœ… å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å…³é—­")

# ==================== å¥åº·æ£€æŸ¥ ====================

@app.get("/")
async def root():
    """æ ¹è·¯ç”± - è¿”å› index.html å‰ç«¯"""
    import os
    index_path = os.path.join(os.path.dirname(__file__), "index.html")
    if os.path.isfile(index_path):
        return FileResponse(index_path, media_type="text/html")
    return {"message": "viper-node-store API", "status": "running", "data_source": "Supabase"}

@app.get("/index.html")
async def index_html():
    """ç›´æ¥è®¿é—® index.html"""
    import os
    index_path = os.path.join(os.path.dirname(__file__), "index.html")
    if os.path.isfile(index_path):
        return FileResponse(index_path, media_type="text/html")
    raise HTTPException(status_code=404, detail="index.html not found")

@app.get("/api/status")
async def status():
    """API çŠ¶æ€æ£€æŸ¥"""
    return {
        "status": "running",
        "version": "2.0.0",
        "data_source": "Supabase",
        "timestamp": datetime.now().isoformat()
    }

# ==================== èŠ‚ç‚¹ API ====================

@app.get("/api/nodes")
async def get_nodes(
    limit: int = Query(None, ge=1, le=500),
    show_free: bool = Query(True),
    show_china: bool = Query(True),
    user_id: Optional[str] = Header(None, alias="X-User-ID")
):
    """
    è·å–èŠ‚ç‚¹åˆ—è¡¨ï¼ˆä» Supabaseï¼‰
    
    å®‰å…¨ç‰¹æ€§ï¼š
    - VIP ç”¨æˆ·å¯è·å–æœ€å¤š 500 ä¸ªèŠ‚ç‚¹
    - é VIP ç”¨æˆ·æœ€å¤šè·å– 20 ä¸ªèŠ‚ç‚¹
    - é™åˆ¶åœ¨æœåŠ¡å™¨ç«¯å®ç°ï¼Œæ— æ³•è¢«å‰ç«¯ç»•è¿‡
    
    Parameters:
    - limit: è¿”å›èŠ‚ç‚¹æ•°é‡é™åˆ¶ï¼ˆ1-500ï¼Œå¯é€‰ï¼‰
    - show_free: æ˜¯å¦æ˜¾ç¤ºå…è´¹èŠ‚ç‚¹
    - show_china: æ˜¯å¦æ˜¾ç¤ºä¸­å›½èŠ‚ç‚¹
    - X-User-ID: ç”¨æˆ·IDï¼ˆHTTP headerï¼‰
    """
    try:
        # æ£€æŸ¥ç”¨æˆ· VIP çŠ¶æ€
        is_vip = await check_user_vip_status(user_id)
        
        # ç¡®å®šè¿”å›çš„èŠ‚ç‚¹æ•°é‡
        if limit is None:
            # å¦‚æœæ²¡æœ‰æŒ‡å®š limitï¼Œä½¿ç”¨é»˜è®¤å€¼
            default_limit = 500 if is_vip else 20
            limit = default_limit
        else:
            # å¦‚æœæŒ‡å®šäº† limitï¼Œé VIP ç”¨æˆ·æœ€å¤š 20 ä¸ª
            if not is_vip and limit > 20:
                limit = 20
        
        logger.info(f"ğŸ“‹ è·å–èŠ‚ç‚¹: VIP={is_vip}, limit={limit}, user_id={user_id or '(anonymous)'}")
        
        nodes = await get_supabase_nodes(limit=limit, show_free=show_free, show_china=show_china)
        return nodes
    except Exception as e:
        logger.error(f"âŒ è·å–èŠ‚ç‚¹å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== åŒæ­¥ä¿¡æ¯ API ====================

@app.get("/api/sync-info")
async def get_sync_info():
    """
    è·å–åŒæ­¥ä¿¡æ¯ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤º"ä¸Šæ¬¡æ›´æ–°äºXåˆ†é’Ÿå‰"ï¼‰
    
    è¿”å›ï¼š
    - last_updated_at: ISOæ ¼å¼æ—¶é—´æˆ³
    - minutes_ago: è·ç¦»ç°åœ¨çš„åˆ†é’Ÿæ•°
    - nodes_count: èŠ‚ç‚¹æ€»æ•°
    - active_count: æ´»è·ƒèŠ‚ç‚¹æ•°ï¼ˆå·²æµ‹è¯•ï¼‰
    - source: æ•°æ®æ¥æºï¼ˆsupabaseï¼‰
    """
    try:
        # è·å–æ‰€æœ‰èŠ‚ç‚¹ç»Ÿè®¡
        nodes = await get_supabase_nodes(limit=10000)
        
        if not nodes:
            return {
                "last_updated_at": datetime.now().isoformat(),
                "minutes_ago": 0,
                "nodes_count": 0,
                "active_count": 0,
                "source": "supabase",
                "sync_metadata": {
                    "total_nodes": 0,
                    "tested_nodes": 0,
                    "pending_test": 0
                }
            }
        
        # ä»èŠ‚ç‚¹ä¸­è·å–æœ€æ–°çš„æ›´æ–°æ—¶é—´
        latest_time = None
        for node in nodes:
            if node.get("updated_at"):
                latest_time = node.get("updated_at")
                break
        
        # è®¡ç®—åˆ†é’Ÿå·®å¼‚
        minutes_ago = 0
        if latest_time:
            try:
                last_synced = datetime.fromisoformat(latest_time.replace('Z', '+00:00'))
                now = datetime.now(last_synced.tzinfo) if last_synced.tzinfo else datetime.now()
                minutes_ago = max(0, int((now - last_synced).total_seconds() / 60))
            except Exception as e:
                logger.debug(f"è®¡ç®—æ—¶é—´å·®å¼‚å¤±è´¥: {e}")
                minutes_ago = 0
        
        # ç»Ÿè®¡èŠ‚ç‚¹
        active_count = len([n for n in nodes if n.get("alive")])
        
        return {
            "last_updated_at": latest_time or datetime.now().isoformat(),
            "minutes_ago": minutes_ago,
            "nodes_count": len(nodes),
            "active_count": active_count,
            "source": "supabase",
            "sync_metadata": {
                "total_nodes": len(nodes),
                "tested_nodes": active_count,
                "pending_test": len(nodes) - active_count
            }
        }
    except Exception as e:
        logger.error(f"âŒ è·å–åŒæ­¥ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
        return {
            "last_updated_at": datetime.now().isoformat(),
            "minutes_ago": 0,
            "nodes_count": 0,
            "active_count": 0,
            "source": "error",
            "error": str(e)
        }

# ==================== æ‰‹åŠ¨è§¦å‘è½®è¯¢ ====================

@app.post("/api/sync/poll-now")
async def trigger_manual_poll(background_tasks = None):
    """
    æ‰‹åŠ¨è§¦å‘è½®è¯¢ï¼ˆå‘ SpiderFlow å‘é€è¯·æ±‚ï¼‰
    æ³¨ï¼šå®é™…æ•°æ®ä»ä» Supabase è¯»å–
    """
    try:
        # å‘ SpiderFlow è§¦å‘è½®è¯¢
        async with aiohttp.ClientSession() as session:
            trigger_url = f"{SPIDERFLOW_API_URL}/api/sync/poll-now"
            try:
                async with session.post(trigger_url, timeout=aiohttp.ClientTimeout(total=5)) as resp:
                    if resp.status == 200:
                        logger.info("âœ… å·²å‘ SpiderFlow å‘é€è½®è¯¢è¯·æ±‚")
                    else:
                        logger.warning(f"âš ï¸  SpiderFlow è½®è¯¢è¿”å› {resp.status}")
            except Exception as e:
                logger.warning(f"âš ï¸  æ— æ³•è¿æ¥ SpiderFlow: {e}")
        
        return {
            "status": "poll_triggered",
            "message": "å·²è¯·æ±‚ SpiderFlow æ‰§è¡Œè½®è¯¢ï¼Œç»“æœå°†ä¿å­˜åˆ° Supabase",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ è§¦å‘è½®è¯¢å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

# ==================== ç²¾ç¡®æµ‹é€Ÿ API ====================

@app.post("/api/nodes/precision-test")
async def precision_speed_test(request: PrecisionTestRequest):
    """
    ç”¨æˆ·å‘èµ·çš„ç²¾ç¡®æµ‹é€Ÿ - çœŸå®ä¸‹è½½æµ‹è¯•
    
    æ³¨æ„ï¼šè¿™é‡Œä¸é€šè¿‡ä»£ç†ä¸‹è½½ï¼Œå› ä¸ºä»£ç†éœ€è¦æœ¬åœ°ä»£ç†è½¯ä»¶æ”¯æŒã€‚
    æ”¹ä¸ºç›´æ¥æµ‹é€ŸæœåŠ¡å™¨é€Ÿåº¦ï¼Œä½œä¸ºèŠ‚ç‚¹æ€§èƒ½çš„å‚è€ƒã€‚
    """
    try:
        test_file_size = request.test_file_size
        
        logger.info(f"âš¡ ç”¨æˆ·å‘èµ·ç²¾ç¡®æµ‹é€Ÿ | æ–‡ä»¶å¤§å°: {test_file_size}MB | ä»£ç†: {request.proxy_url}")
        
        # ç”Ÿæˆæµ‹è¯•æ–‡ä»¶URLï¼ˆç›´æ¥ä»æµ‹é€ŸæœåŠ¡å™¨ä¸‹è½½ï¼Œä¸é€šè¿‡ä»£ç†ï¼‰
        # å› ä¸ºä»£ç†éœ€è¦æœ¬åœ°å®¢æˆ·ç«¯æ”¯æŒï¼Œåç«¯æ— æ³•ç›´æ¥ä½¿ç”¨è¿œç¨‹ä»£ç†
        test_file_url = f"https://speed.cloudflare.com/__down?bytes={test_file_size * 1024 * 1024}"
        
        start_time = time.time()
        bytes_downloaded = 0
        
        try:
            # ä½¿ç”¨å¸¦è¶…æ—¶çš„ aiohttp ä¼šè¯è¿›è¡Œä¸‹è½½
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    test_file_url, 
                    timeout=aiohttp.ClientTimeout(total=120, connect=10, sock_read=30),
                    ssl=False  # è·³è¿‡SSLéªŒè¯ä»¥é¿å…ç½‘ç»œé—®é¢˜
                ) as resp:
                    if resp.status == 200:
                        async for chunk in resp.content.iter_chunked(8192):
                            bytes_downloaded += len(chunk)
                    else:
                        logger.error(f"HTTP {resp.status} from {test_file_url}")
                        raise Exception(f"HTTP {resp.status}")
            
            download_time = time.time() - start_time
            
            if download_time <= 0:
                download_time = 0.001
            
            # è®¡ç®—é€Ÿåº¦ (MB/s)
            speed_mbps = (bytes_downloaded / (1024 * 1024)) / download_time
            
            logger.info(f"âœ… ç²¾ç¡®æµ‹é€Ÿå®Œæˆ | å¤§å°: {bytes_downloaded/(1024*1024):.1f}MB | æ—¶é—´: {download_time:.1f}s | é€Ÿåº¦: {speed_mbps:.1f}MB/s")
            
            return {
                "status": "success",
                "speed_mbps": round(speed_mbps, 2),
                "download_time_seconds": round(download_time, 2),
                "traffic_consumed_mb": round(bytes_downloaded / (1024 * 1024), 2),
                "bytes_downloaded": bytes_downloaded,
                "test_file_size_requested_mb": test_file_size,
                "message": f"ç²¾ç¡®æµ‹é€Ÿå®Œæˆ: {speed_mbps:.1f} MB/s",
                "timestamp": datetime.now().isoformat()
            }
        except asyncio.TimeoutError:
            logger.error(f"ç²¾ç¡®æµ‹é€Ÿè¶…æ—¶ (> 120ç§’)")
            return {
                "status": "timeout",
                "speed_mbps": 0,
                "message": "æµ‹é€Ÿè¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•",
                "timestamp": datetime.now().isoformat()
            }
        except Exception as inner_err:
            logger.error(f"ç²¾ç¡®æµ‹é€Ÿä¸‹è½½å¤±è´¥: {inner_err}")
            if bytes_downloaded > 0:
                download_time = time.time() - start_time
                if download_time <= 0:
                    download_time = 0.001
                speed_mbps = (bytes_downloaded / (1024 * 1024)) / download_time
                return {
                    "status": "partial_success",
                    "speed_mbps": round(speed_mbps, 2),
                    "download_time_seconds": round(download_time, 2),
                    "traffic_consumed_mb": round(bytes_downloaded / (1024 * 1024), 2),
                    "message": f"éƒ¨åˆ†æµ‹è¯•: {speed_mbps:.1f} MB/s",
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {
                    "status": "error",
                    "speed_mbps": 0,
                    "latency": 9999,
                    "message": f"æµ‹é€Ÿå¤±è´¥: æ— æ³•è¿æ¥åˆ°æµ‹é€ŸæœåŠ¡å™¨",
                    "timestamp": datetime.now().isoformat()
                }
    except Exception as e:
        logger.error(f"ç²¾ç¡®æµ‹é€Ÿå¼‚å¸¸: {e}")
        return {
            "status": "error",
            "speed_mbps": 0,
            "message": f"API é”™è¯¯: {str(e)[:50]}",
            "timestamp": datetime.now().isoformat()
        }

@app.post("/api/nodes/latency-test")
async def latency_test(request: LatencyTestRequest):
    """
    å»¶è¿Ÿæµ‹è¯• - ç®€å•çš„ ping å»¶è¿Ÿæµ‹è¯•
    """
    try:
        proxy_url = request.proxy_url
        
        logger.info(f"âš¡ æ‰§è¡Œå»¶è¿Ÿæµ‹è¯•")
        
        start_time = time.time()
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.head(proxy_url, timeout=aiohttp.ClientTimeout(total=10), allow_redirects=False) as resp:
                    latency = int((time.time() - start_time) * 1000)  # æ¯«ç§’
                    
                    return {
                        "status": "success",
                        "latency": latency,
                        "latency_ms": latency,
                        "timestamp": datetime.now().isoformat()
                    }
        except asyncio.TimeoutError:
            return {
                "status": "timeout",
                "latency": 9999,
                "message": "å»¶è¿Ÿæµ‹è¯•è¶…æ—¶",
                "timestamp": datetime.now().isoformat()
            }
        except Exception as e:
            logger.warning(f"å»¶è¿Ÿæµ‹è¯•å¤±è´¥: {e}")
            return {
                "status": "error",
                "latency": 9999,
                "message": str(e),
                "timestamp": datetime.now().isoformat()
            }
    except Exception as e:
        logger.error(f"å»¶è¿Ÿæµ‹è¯•å¼‚å¸¸: {e}")
        return {
            "status": "error",
            "latency": 9999,
            "message": f"API é”™è¯¯: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }

# ==================== æ¿€æ´»ç å…‘æ¢ API ====================

class RedeemCodeRequest(BaseModel):
    """æ¿€æ´»ç å…‘æ¢è¯·æ±‚"""
    code: str
    user_id: str  # Supabase ç”¨æˆ· ID

@app.post("/api/auth/redeem-code")
async def redeem_code(request: RedeemCodeRequest):
    """
    å…‘æ¢æ¿€æ´»ç å‡çº§åˆ° VIP
    
    æ¿€æ´»ç æ ¼å¼ï¼šVIPX-XXXX-XXXXï¼ˆç¤ºä¾‹ï¼‰
    æ¿€æ´»ç æœ‰æ•ˆæœŸï¼šæ ¹æ®æ¿€æ´»ç é…ç½®å†³å®š
    """
    try:
        code = request.code.strip().upper()
        user_id = request.user_id
        
        if not code or not user_id:
            return {
                "status": "error",
                "message": "æ¿€æ´»ç å’Œç”¨æˆ·IDä¸èƒ½ä¸ºç©º"
            }
        
        logger.info(f"ğŸ”‘ å…‘æ¢æ¿€æ´»ç : code={code}, user_id={user_id}")
        
        # åˆå§‹åŒ– Supabase å®¢æˆ·ç«¯
        supabase_client = supabase.create_client(SUPABASE_URL, SUPABASE_KEY)
        
        # æŸ¥è¯¢ activation_codes è¡¨
        try:
            codes_result = supabase_client.table("activation_codes").select("*").eq("code", code).execute()
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢æ¿€æ´»ç è¡¨å¤±è´¥: {e}")
            return {
                "status": "error",
                "message": "ç³»ç»Ÿé”™è¯¯ï¼šæ— æ³•æŸ¥è¯¢æ¿€æ´»ç "
            }
        
        if not codes_result.data:
            logger.warning(f"âŒ æ¿€æ´»ç ä¸å­˜åœ¨: {code}")
            return {
                "status": "error",
                "message": "æ¿€æ´»ç ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸ"
            }
        
        code_record = codes_result.data[0]
        
        # æ£€æŸ¥æ¿€æ´»ç æ˜¯å¦å·²è¢«ä½¿ç”¨
        if code_record.get("used"):
            logger.warning(f"âŒ æ¿€æ´»ç å·²è¢«ä½¿ç”¨: {code}")
            return {
                "status": "error",
                "message": "è¯¥æ¿€æ´»ç å·²è¢«å…‘æ¢"
            }
        
        # æ£€æŸ¥æ¿€æ´»ç æ˜¯å¦è¿‡æœŸ
        if code_record.get("expires_at"):
            try:
                expires_at = datetime.fromisoformat(code_record["expires_at"].replace("Z", "+00:00"))
                if expires_at < datetime.now(expires_at.tzinfo):
                    logger.warning(f"âŒ æ¿€æ´»ç å·²è¿‡æœŸ: {code}")
                    return {
                        "status": "error",
                        "message": "æ¿€æ´»ç å·²è¿‡æœŸ"
                    }
            except:
                pass  # å¦‚æœæ—¶é—´è§£æå¤±è´¥ï¼Œç»§ç»­å¤„ç†
        
        # è·å– VIP æ—¶é•¿ï¼ˆå¤©æ•°ï¼‰
        vip_days = code_record.get("vip_days", 30)  # é»˜è®¤ 30 å¤©
        
        # è®¡ç®— VIP è¿‡æœŸæ—¶é—´
        vip_until = datetime.utcnow() + timedelta(days=vip_days)
        
        # æ›´æ–°ç”¨æˆ·çš„ vip_until å­—æ®µ
        # ä½¿ç”¨ upsert ç¡®ä¿å³ä½¿å­—æ®µä¸å­˜åœ¨ä¹Ÿèƒ½æˆåŠŸï¼ˆSupabase ä¼šè‡ªåŠ¨æ·»åŠ ï¼‰
        try:
            # é¦–å…ˆå°è¯•ç›´æ¥æ›´æ–°
            profiles_result = supabase_client.table("profiles").update({
                "vip_until": vip_until.isoformat()
            }).eq("id", user_id).execute()
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ›´æ–°
            if profiles_result.data:
                logger.info(f"âœ… ç”¨æˆ· VIP çŠ¶æ€å·²æ›´æ–°: {user_id}")
            else:
                # å¦‚æœæ²¡æœ‰è¿”å›æ•°æ®ï¼Œå¯èƒ½æ˜¯å› ä¸ºç”¨æˆ·ä¸å­˜åœ¨æˆ– RLS é™åˆ¶
                # å°è¯•æ’å…¥æˆ–æ›´æ–°ï¼ˆupsertï¼‰
                logger.warning(f"âš ï¸ ç›´æ¥æ›´æ–°å¤±è´¥ï¼Œå°è¯• upsert: {user_id}")
                
                # ä½¿ç”¨ upsertï¼šå¦‚æœç”¨æˆ·ä¸å­˜åœ¨ï¼Œåˆ›å»ºï¼›å¦‚æœå­˜åœ¨ï¼Œæ›´æ–°
                upsert_result = supabase_client.table("profiles").upsert({
                    "id": user_id,
                    "vip_until": vip_until.isoformat()
                }).execute()
                
                if not upsert_result.data:
                    logger.error(f"âŒ upsert ä¹Ÿå¤±è´¥äº†: {user_id}")
                    return {
                        "status": "error",
                        "message": "æ›´æ–° VIP çŠ¶æ€å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
                    }
                
                logger.info(f"âœ… ç”¨æˆ· VIP çŠ¶æ€å·²é€šè¿‡ upsert æ›´æ–°: {user_id}")
                
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°ç”¨æˆ· VIP çŠ¶æ€å¼‚å¸¸: {e}")
            return {
                "status": "error",
                "message": f"æ›´æ–° VIP çŠ¶æ€å¤±è´¥: {str(e)}"
            }
        
        # æ ‡è®°æ¿€æ´»ç ä¸ºå·²ä½¿ç”¨
        try:
            supabase_client.table("activation_codes").update({
                "used": True,
                "used_by": user_id,
                "used_at": datetime.utcnow().isoformat()
            }).eq("code", code).execute()
        except Exception as e:
            logger.warning(f"âš ï¸ æ ‡è®°æ¿€æ´»ç å¤±è´¥ï¼ˆä½†ç”¨æˆ·å·²å‡çº§ï¼‰: {e}")
            # ä¸ä¸­æ–­æµç¨‹ï¼Œå› ä¸ºç”¨æˆ·å·²ç»å‡çº§äº†
        
        logger.info(f"âœ… æ¿€æ´»ç å…‘æ¢æˆåŠŸ: {code}, VIP è‡³ {vip_until.isoformat()}")
        
        return {
            "status": "success",
            "message": f"æ­å–œï¼æ‚¨å·²å‡çº§ä¸º VIP ç”¨æˆ·ï¼Œæœ‰æ•ˆæœŸè‡³ {vip_until.strftime('%Y-%m-%d')}",
            "vip_until": vip_until.isoformat()
        }
        
    except Exception as e:
        logger.error(f"âŒ æ¿€æ´»ç å…‘æ¢å¼‚å¸¸: {e}")
        return {
            "status": "error",
            "message": f"å…‘æ¢å¤±è´¥: {str(e)}"
        }

# ==================== SpiderFlow API ä»£ç† ====================

@app.get("/api/proxy/nodes")
async def proxy_nodes(
    limit: int = Query(500, ge=1, le=500),
    show_socks_http: bool = Query(False),
    show_china_nodes: bool = Query(False)
):
    """ä»£ç† SpiderFlow çš„ /api/nodes è¯·æ±‚"""
    try:
        async with aiohttp.ClientSession() as session:
            url = f"{SPIDERFLOW_API_URL}/api/nodes"
            params = {
                "limit": limit,
                "show_socks_http": show_socks_http,
                "show_china_nodes": show_china_nodes
            }
            async with session.get(url, params=params, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                return await resp.json()
    except Exception as e:
        logger.error(f"âŒ ä»£ç† SpiderFlow èŠ‚ç‚¹æ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=502, detail=f"SpiderFlow æœåŠ¡ä¸å¯ç”¨: {str(e)}")

@app.get("/api/proxy/system/stats")
async def proxy_system_stats():
    """ä»£ç† SpiderFlow çš„ /api/system/stats è¯·æ±‚"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{SPIDERFLOW_API_URL}/api/system/stats", timeout=aiohttp.ClientTimeout(total=10)) as resp:
                return await resp.json()
    except Exception as e:
        logger.error(f"âŒ ä»£ç† SpiderFlow ç³»ç»Ÿç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=502, detail=f"SpiderFlow æœåŠ¡ä¸å¯ç”¨: {str(e)}")

@app.get("/api/proxy/nodes/stats")
async def proxy_nodes_stats():
    """ä»£ç† SpiderFlow çš„ /nodes/stats è¯·æ±‚"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(f"{SPIDERFLOW_API_URL}/nodes/stats", timeout=aiohttp.ClientTimeout(total=10)) as resp:
                return await resp.json()
    except Exception as e:
        logger.error(f"âŒ ä»£ç† SpiderFlow èŠ‚ç‚¹ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=502, detail=f"SpiderFlow æœåŠ¡ä¸å¯ç”¨: {str(e)}")

# ==================== èŠ‚ç‚¹å¥åº·æ£€æµ‹ API ====================

@app.post("/api/health-check")
async def trigger_health_check(request: HealthCheckRequest = None):
    """
    æ‰‹åŠ¨è§¦å‘èŠ‚ç‚¹å¥åº·æ£€æµ‹
    
    ç”±å‰ç«¯ã€ŒğŸ¥ å¥åº·æ£€æµ‹ã€æŒ‰é’®è°ƒç”¨
    æ¯æ¬¡æ£€æµ‹ä¸€æ‰¹èŠ‚ç‚¹ï¼Œæ›´æ–°å…¶åœ¨çº¿çŠ¶æ€åˆ°æ•°æ®åº“
    Vercel ç¯å¢ƒå»ºè®® batch_size=30-50ï¼ˆå—æ‰§è¡Œæ—¶é—´é™åˆ¶ï¼‰
    
    æ³¨æ„ï¼šVercel Hobby å…è´¹è®¡åˆ’ä¸æ”¯æŒ Cron Jobsï¼Œéœ€æ‰‹åŠ¨è§¦å‘
    å¦‚éœ€å®šæ—¶æ£€æµ‹ï¼Œå¯ä½¿ç”¨å…è´¹æœåŠ¡å¦‚ cron-job.org å®šæ—¶è°ƒç”¨æ­¤ API
    
    Returns:
        æ£€æµ‹ç»“æœç»Ÿè®¡
    """
    try:
        batch_size = request.batch_size if request else 100
        
        logger.info(f"ğŸ¥ æ”¶åˆ°å¥åº·æ£€æµ‹è¯·æ±‚ (batch_size={batch_size})")
        logger.info(f"SUPABASE_URL: {SUPABASE_URL[:50] if SUPABASE_URL else 'NOT SET'}...")
        logger.info(f"SUPABASE_KEY: {SUPABASE_KEY[:20] if SUPABASE_KEY else 'NOT SET'}...")
        
        # å¯¼å…¥å¥åº·æ£€æµ‹æ¨¡å—
        from health_checker import run_health_check, LightweightHealthChecker, SupabaseHealthUpdater
        from health_checker import NodeStatus
        from datetime import datetime as dt
        
        # 1. å…ˆä» app_fastapi çš„ get_supabase_nodes è·å–èŠ‚ç‚¹
        logger.info("ğŸ“¡ ä½¿ç”¨ app_fastapi çš„æ–¹å¼è·å–èŠ‚ç‚¹...")
        nodes = await get_supabase_nodes(limit=batch_size, show_free=True, show_china=True)
        
        logger.info(f"âœ… è·å–åˆ° {len(nodes)} ä¸ªèŠ‚ç‚¹")
        
        if not nodes:
            logger.warning("âŒ æ²¡æœ‰èŠ‚ç‚¹å¯æ£€æµ‹")
            return {
                "status": "success",
                "data": {
                    "status": "no_nodes",
                    "checked_count": 0,
                    "online_count": 0,
                    "offline_count": 0,
                    "suspect_count": 0,
                    "duration_seconds": 0
                },
                "timestamp": datetime.now().isoformat()
            }
        
        # 2. æ‰§è¡Œå¥åº·æ£€æµ‹
        logger.info("ğŸ¥ å¼€å§‹æ£€æµ‹èŠ‚ç‚¹...")
        checker = LightweightHealthChecker(
            tcp_timeout=5.0,
            http_timeout=8.0,
            max_retries=2,
            max_concurrent=20
        )
        
        # å°†èŠ‚ç‚¹æ•°æ®è½¬æ¢ä¸ºæ£€æµ‹æ ¼å¼
        check_nodes = []
        for node in nodes:
            check_nodes.append({
                "id": node.get("id", ""),
                "host": node.get("host", ""),
                "port": node.get("port", 0),
                "protocol": node.get("protocol", "unknown"),
                "name": node.get("name", "")
            })
        
        # æ‰§è¡Œæ‰¹é‡æ£€æµ‹
        results = await checker.check_nodes_batch(check_nodes)
        
        # 3. ç»Ÿè®¡ç»“æœ
        online_count = sum(1 for r in results if r.status == NodeStatus.ONLINE)
        offline_count = sum(1 for r in results if r.status == NodeStatus.OFFLINE)
        suspect_count = sum(1 for r in results if r.status == NodeStatus.SUSPECT)
        
        logger.info(f"ğŸ“Š æ£€æµ‹ç»“æœ: åœ¨çº¿={online_count}, ç¦»çº¿={offline_count}, å¯ç–‘={suspect_count}")
        
        # 4. æ›´æ–°æ•°æ®åº“
        logger.info("ğŸ’¾ æ›´æ–°æ•°æ®åº“...")
        updater = SupabaseHealthUpdater(supabase_url=SUPABASE_URL, supabase_key=SUPABASE_KEY)
        success, fail = await updater.update_node_status(results)
        logger.info(f"âœ… æ•°æ®åº“æ›´æ–°: æˆåŠŸ={success}, å¤±è´¥={fail}")
        
        # è·å–é—®é¢˜èŠ‚ç‚¹åˆ—è¡¨
        problem_nodes = [
            {
                "id": r.node_id,
                "name": r.host,
                "host": r.host,
                "port": r.port,
                "status": r.status.value
            }
            for r in results if r.status in [NodeStatus.OFFLINE, NodeStatus.SUSPECT]
        ]
        
        return {
            "status": "success",
            "data": {
                "status": "completed",
                "total": len(results),
                "online": online_count,
                "offline": offline_count,
                "suspect": suspect_count,
                "problem_nodes": problem_nodes,
                "update_success": success,
                "update_fail": fail
            },
            "timestamp": datetime.now().isoformat()
        }
        
    except ImportError as e:
        logger.error(f"âŒ å¥åº·æ£€æµ‹æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "status": "error",
            "message": "å¥åº·æ£€æµ‹æ¨¡å—æœªå®‰è£…",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"âŒ å¥åº·æ£€æµ‹å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return {
            "status": "error",
            "message": str(e),
            "timestamp": datetime.now().isoformat()
        }

# æ³¨æ„ï¼šVercel Cron Jobs ä»…åœ¨ Pro åŠä»¥ä¸Šè®¡åˆ’æ”¯æŒ
# Hobby å…è´¹è®¡åˆ’ï¼šä½¿ç”¨å‰ç«¯æŒ‰é’®æ‰‹åŠ¨è§¦å‘
# å¦‚éœ€å®šæ—¶ä»»åŠ¡ï¼Œå¯ä½¿ç”¨å¤–éƒ¨å…è´¹æœåŠ¡ï¼ˆå¦‚ cron-job.orgï¼‰å®šæ—¶è°ƒç”¨ /api/health-check

@app.get("/api/health-check/stats")
async def get_health_stats():
    """
    è·å–å¥åº·æ£€æµ‹ç»Ÿè®¡æ•°æ®
    
    è¿”å›å„çŠ¶æ€èŠ‚ç‚¹çš„æ•°é‡ç»Ÿè®¡
    """
    try:
        # ä» Supabase æŸ¥è¯¢ç»Ÿè®¡
        url = f"{SUPABASE_URL}/rest/v1/nodes?select=status"
        
        headers = {
            "apikey": SUPABASE_KEY,
            "Authorization": f"Bearer {SUPABASE_KEY}",
            "Content-Type": "application/json"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, timeout=aiohttp.ClientTimeout(total=10)) as resp:
                if resp.status == 200:
                    rows = await resp.json()
                    
                    # ç»Ÿè®¡å„çŠ¶æ€æ•°é‡
                    stats = {
                        "total": len(rows),
                        "online": 0,
                        "offline": 0,
                        "suspect": 0,
                        "unknown": 0
                    }
                    
                    for row in rows:
                        status = row.get("status", "unknown")
                        if status in stats:
                            stats[status] += 1
                        else:
                            stats["unknown"] += 1
                    
                    return {
                        "status": "success",
                        "data": stats,
                        "timestamp": datetime.now().isoformat()
                    }
                else:
                    return {
                        "status": "error",
                        "message": f"æŸ¥è¯¢å¤±è´¥: HTTP {resp.status}",
                        "timestamp": datetime.now().isoformat()
                    }
                    
    except Exception as e:
        logger.error(f"âŒ è·å–å¥åº·ç»Ÿè®¡å¤±è´¥: {e}")
        return {
            "status": "error",
            "message": str(e),
            "timestamp": datetime.now().isoformat()
        }

# ==================== ä¸»ç¨‹åº ====================

if __name__ == "__main__":
    import uvicorn
    
    logger.info("=" * 60)
    logger.info("å¯åŠ¨ viper-node-store API æœåŠ¡ (Supabase ç‰ˆæœ¬)")
    logger.info("=" * 60)
    
    uvicorn.run(
        "app_fastapi:app",
        host="0.0.0.0",
        port=8002,
        reload=False,
        log_level="info"
    )
